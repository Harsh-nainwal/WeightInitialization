{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7379e7a8-0979-4bdb-bb6b-f28653490a7c",
   "metadata": {},
   "source": [
    "\n",
    "Part 1: Understanding Weight Initialization\n",
    "\n",
    "Question: Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialize the weights carefully?\n",
    "\n",
    "ANS:\n",
    "Weight initialization is crucial in artificial neural networks because it sets the starting point for model optimization during training. Proper initialization helps to ensure that the network learns effectively and converges to a good solution. Careful initialization is necessary because:\n",
    "\n",
    "Avoiding Symmetry: If all weights are initialized to the same value, each neuron in a layer will compute the same output during forward propagation, leading to symmetry between neurons. This symmetry breaks the representational power of the network and inhibits learning.\n",
    "\n",
    "Preventing Saturation: Improper initialization can cause neurons to saturate, where they become stuck at extreme values due to activation functions like sigmoid or tanh. This slows down learning and makes it harder for the network to update weights effectively.\n",
    "\n",
    "Facilitating Gradient Flow: Well-initialized weights ensure that gradients propagate effectively through the network during backpropagation. This helps to prevent issues like vanishing or exploding gradients, which can hinder convergence.\n",
    "\n",
    "Question: Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "ANS:\n",
    "Improper weight initialization can lead to several challenges that affect model training and convergence:\n",
    "\n",
    "Vanishing/Exploding Gradients: If weights are initialized too small or too large, gradients can vanish (become extremely small) or explode (become extremely large) as they propagate backward through the network. This makes it difficult for the model to learn effectively, as updates to the weights become negligible or overly large.\n",
    "\n",
    "Symmetry and Redundancy: If weights are initialized identically, neurons in the same layer will compute the same outputs during forward propagation, leading to symmetry. This reduces the model's capacity to learn unique features from the data and can result in redundancy in the network's representations.\n",
    "\n",
    "Slow Convergence: Improperly initialized weights can lead to slow convergence during training. The optimization process may get stuck in local minima or struggle to find an optimal solution due to ineffective weight updates.\n",
    "\n",
    "Question: Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "ANS:\n",
    "Variance refers to the spread or dispersion of values in a dataset or distribution. In the context of weight initialization, variance plays a crucial role in determining the range of values that weights can take on. It is essential to consider the variance of weights during initialization because:\n",
    "\n",
    "Impact on Activation: The variance of weights directly influences the spread of activations in the network. Higher variance can lead to more diverse activations, allowing the network to capture a broader range of features from the data.\n",
    "\n",
    "Gradient Stability: Proper variance ensures that gradients neither vanish nor explode during backpropagation. Balancing the variance of weights helps to maintain stable gradients throughout the network, facilitating efficient training.\n",
    "\n",
    "Avoiding Saturation: Variance affects the likelihood of neurons saturating, where their outputs become stuck at extreme values. Properly adjusted variance can help prevent saturation and ensure that neurons operate in the regions of their activation functions where gradients are most informative.\n",
    "\n",
    "Part 2: Weight Initialization Techniques\n",
    "\n",
    "Question: Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "\n",
    "ANS:\n",
    "Zero initialization involves setting all weights in the network to zero initially. While simple, zero initialization has limitations:\n",
    "\n",
    "Symmetry: All weights being the same leads to symmetry between neurons, hindering learning.\n",
    "\n",
    "Vanishing Gradients: Zero initialization can cause gradients to vanish, especially in deeper networks, as the same gradient signal is propagated backward through layers.\n",
    "\n",
    "Zero initialization can be appropriate when using activation functions like ReLU, which are not affected by vanishing gradients and when employing techniques like batch normalization or skip connections to mitigate symmetry issues.\n",
    "\n",
    "Question: Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients?\n",
    "ANS:\n",
    "\n",
    "Random initialization involves setting weights to random values sampled from a specified distribution, such as uniform or Gaussian. To mitigate saturation or vanishing/exploding gradients:\n",
    "\n",
    "Xavier/Glorot Initialization: Scaling weights based on the number of input and output units can help balance the variance of activations and gradients, reducing the likelihood of saturation or vanishing/exploding gradients.\n",
    "\n",
    "He Initialization: Similar to Xavier initialization but considers only the number of input units, which can be more suitable for activation functions like ReLU that may lead to vanishing gradients with Xavier initialization.\n",
    "\n",
    "By properly scaling the initial weights based on the network architecture and activation functions, random initialization can help alleviate these issues.\n",
    "\n",
    "Question: Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it.\n",
    "\n",
    "ANS:\n",
    "Xavier/Glorot initialization sets the weights using a distribution with zero mean and a variance calculated based on the number of input and output units. It addresses challenges by:\n",
    "\n",
    "Balanced Variance: Xavier initialization scales weights to ensure that the variance of activations remains consistent across layers. This helps prevent saturation and ensures that gradients neither vanish nor explode during training.\n",
    "\n",
    "Effective Learning: By maintaining stable gradients, Xavier initialization enables more effective learning, leading to faster convergence and better generalization performance.\n",
    "\n",
    "The underlying theory behind Xavier initialization is to ensure that the activations and gradients have appropriate variance, facilitating efficient information flow through the network during both forward and backward passes.\n",
    "\n",
    "Question: Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n",
    "\n",
    "ANS:\n",
    "He initialization, proposed by Kaiming He et al., initializes weights using a Gaussian distribution with zero mean and a variance proportional to the number of input units. It differs from Xavier initialization in that it only considers the number of input units, not the average of input and output units. He initialization is preferred:\n",
    "\n",
    "For ReLU Activation: He initialization is tailored for activation functions like ReLU, which can lead to vanishing gradients with Xavier initialization due to the zero-centered nature of ReLU.\n",
    "\n",
    "Deeper Networks: He initialization is more suitable for deeper networks, as it maintains higher variances in activations and gradients, preventing saturation and ensuring effective learning even in deep architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8ae01b-5540-43f5-a1f4-0a5cf742cfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-12 06:44:35.247687: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 06:44:35.252427: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-12 06:44:35.325873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 06:44:36.509801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Zero initialization model...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4742 - loss: 0.6932 - val_accuracy: 0.5350 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4911 - loss: 0.6932 - val_accuracy: 0.4650 - val_loss: 0.6932\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5070 - loss: 0.6931 - val_accuracy: 0.4650 - val_loss: 0.6934\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4849 - loss: 0.6933 - val_accuracy: 0.4650 - val_loss: 0.6934\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5181 - loss: 0.6930 - val_accuracy: 0.4650 - val_loss: 0.6935\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4909 - loss: 0.6933 - val_accuracy: 0.4650 - val_loss: 0.6935\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4708 - loss: 0.6935 - val_accuracy: 0.4650 - val_loss: 0.6935\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4863 - loss: 0.6933 - val_accuracy: 0.4650 - val_loss: 0.6936\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5014 - loss: 0.6932 - val_accuracy: 0.4650 - val_loss: 0.6937\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5028 - loss: 0.6931 - val_accuracy: 0.4650 - val_loss: 0.6938\n",
      "Training Random initialization model...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6977 - loss: 0.6835 - val_accuracy: 0.8100 - val_loss: 0.6420\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8382 - loss: 0.6025 - val_accuracy: 0.8300 - val_loss: 0.4864\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8564 - loss: 0.4169 - val_accuracy: 0.8550 - val_loss: 0.3796\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8963 - loss: 0.2970 - val_accuracy: 0.8700 - val_loss: 0.3566\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.2864 - val_accuracy: 0.8800 - val_loss: 0.3548\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8857 - loss: 0.2854 - val_accuracy: 0.8750 - val_loss: 0.3550\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8911 - loss: 0.2832 - val_accuracy: 0.8750 - val_loss: 0.3577\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8998 - loss: 0.2590 - val_accuracy: 0.8750 - val_loss: 0.3568\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9129 - loss: 0.2551 - val_accuracy: 0.8650 - val_loss: 0.3597\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8949 - loss: 0.2527 - val_accuracy: 0.8550 - val_loss: 0.3580\n",
      "Training Xavier initialization model...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6229 - loss: 0.6496 - val_accuracy: 0.7650 - val_loss: 0.5398\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8374 - loss: 0.4633 - val_accuracy: 0.8050 - val_loss: 0.4523\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8754 - loss: 0.3717 - val_accuracy: 0.8350 - val_loss: 0.3905\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8914 - loss: 0.3084 - val_accuracy: 0.8600 - val_loss: 0.3744\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8975 - loss: 0.2810 - val_accuracy: 0.8500 - val_loss: 0.3793\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8808 - loss: 0.3155 - val_accuracy: 0.8650 - val_loss: 0.3744\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.2696 - val_accuracy: 0.8550 - val_loss: 0.3743\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8903 - loss: 0.2868 - val_accuracy: 0.8450 - val_loss: 0.3831\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8983 - loss: 0.2551 - val_accuracy: 0.8650 - val_loss: 0.3714\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9211 - loss: 0.2179 - val_accuracy: 0.8550 - val_loss: 0.3782\n",
      "Training He initialization model...\n",
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6372 - loss: 0.6338 - val_accuracy: 0.7700 - val_loss: 0.5228\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8248 - loss: 0.4517 - val_accuracy: 0.8150 - val_loss: 0.4181\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8993 - loss: 0.3273 - val_accuracy: 0.8300 - val_loss: 0.3954\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8836 - loss: 0.3291 - val_accuracy: 0.8400 - val_loss: 0.3718\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8617 - loss: 0.3358 - val_accuracy: 0.8400 - val_loss: 0.3609\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8987 - loss: 0.2879 - val_accuracy: 0.8500 - val_loss: 0.3530\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9005 - loss: 0.2889 - val_accuracy: 0.8400 - val_loss: 0.3573\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9167 - loss: 0.2362 - val_accuracy: 0.8450 - val_loss: 0.3531\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9105 - loss: 0.2478 - val_accuracy: 0.8500 - val_loss: 0.3543\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9041 - loss: 0.2423 - val_accuracy: 0.8500 - val_loss: 0.3534\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4868 - loss: 0.6934 \n",
      "Zero initialization model - Test Loss: 0.6937982439994812, Test Accuracy: 0.4650000035762787\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8505 - loss: 0.3765 \n",
      "Random initialization model - Test Loss: 0.35804954171180725, Test Accuracy: 0.8550000190734863\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8464 - loss: 0.3943 \n",
      "Xavier initialization model - Test Loss: 0.37821197509765625, Test Accuracy: 0.8550000190734863\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8343 - loss: 0.3906 \n",
      "He initialization model - Test Loss: 0.3534441292285919, Test Accuracy: 0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, initializers\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(initializer):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_initializer=initializer, input_shape=(20,)))\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_initializer=initializer))\n",
    "    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=initializer))\n",
    "    return model\n",
    "\n",
    "# Initialize models with different initialization techniques\n",
    "zero_model = create_model(initializer='zeros')\n",
    "random_model = create_model(initializer='random_normal')\n",
    "xavier_model = create_model(initializer='glorot_normal')\n",
    "he_model = create_model(initializer='he_normal')\n",
    "\n",
    "# Compile models\n",
    "for model in [zero_model, random_model, xavier_model, he_model]:\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train models\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "for model, name in zip([zero_model, random_model, xavier_model, he_model], ['Zero', 'Random', 'Xavier', 'He']):\n",
    "    print(f'Training {name} initialization model...')\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate models\n",
    "for model, name in zip([zero_model, random_model, xavier_model, he_model], ['Zero', 'Random', 'Xavier', 'He']):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'{name} initialization model - Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890567a6-93d7-4cec-95a3-cd874f0fce71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
